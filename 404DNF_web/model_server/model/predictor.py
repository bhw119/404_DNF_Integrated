import easyocr
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
from torch_geometric.data import Data
import numpy as np
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import LabelEncoder
import json
import os
import sys
import pandas as pd
import re
from model.resgcn import ResGCN

# stdout ë²„í¼ë§ ë¹„í™œì„±í™” (ë¡œê·¸ ì¦‰ì‹œ ì¶œë ¥)
sys.stdout.reconfigure(line_buffering=True)

# í˜„ìž¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# OCR ì—”ì§„ ì´ˆê¸°í™” (ì´ë¯¸ì§€ ë¶„ì„ìš©)
reader = easyocr.Reader(['en', 'ko'])

# ë²ˆì—­ ëª¨ë¸ ë¡œë“œ (ì˜->í•œ, ì´ë¯¸ì§€ ë¶„ì„ìš©)
trans_model_name = "Helsinki-NLP/opus-mt-ko-en"
trans_tokenizer = AutoTokenizer.from_pretrained(trans_model_name)
trans_model = AutoModelForSeq2SeqLM.from_pretrained(trans_model_name).to(device)

# ResGCN ëª¨ë¸ ë¡œë“œ (ë…¸íŠ¸ë¶ êµ¬ì¡° ê¸°ë°˜)
# SentenceTransformer ë¡œë“œ (ìž„ë² ë”© ìƒì„±ìš©)
st_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)
print(f"âœ… SentenceTransformer ë¡œë“œ ì™„ë£Œ (device: {device})")

# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
model_path = os.path.join(MODEL_DIR, "resgcn_improved.pt")
embeddings_path = os.path.join(MODEL_DIR, "embeddings_improved.npy")
meta_path = os.path.join(MODEL_DIR, "embeddings_meta.json")

# embeddings_meta.json ë¡œë“œ
if os.path.exists(meta_path):
    with open(meta_path, 'r', encoding='utf-8') as f:
        meta = json.load(f)
    print(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {meta_path}")
    print(f"   - knn_k: {meta.get('knn_k', 10)}")
    print(f"   - mutual_knn: {meta.get('mutual_knn', True)}")
    print(f"   - metric: {meta.get('metric', 'cosine')}")
    print(f"   - classes: {len(meta.get('classes', []))}ê°œ")
else:
    print(f"âš ï¸  ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {meta_path}")
    print("   ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    meta = {
        'knn_k': 10,
        'mutual_knn': True,
        'metric': 'cosine',
        'classes': []
    }

# Train embeddings ë¡œë“œ (inductive inferenceìš©)
if os.path.exists(embeddings_path):
    X_train = np.load(embeddings_path)
    print(f"âœ… Train embeddings ë¡œë“œ ì™„ë£Œ: {embeddings_path}")
    print(f"   - Shape: {X_train.shape}")
else:
    print(f"âš ï¸  Train embeddings íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {embeddings_path}")
    print("   ë‹¨ì¼ ë…¸ë“œ ê·¸ëž˜í”„ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤ (ê¶Œìž¥í•˜ì§€ ì•ŠìŒ).")
    X_train = None

# ResGCN ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
print(f"ðŸ“¦ ResGCN ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {model_path}")
ckpt = torch.load(model_path, map_location=device)

# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ
if 'hp' in ckpt:
    hp = ckpt['hp']
    in_dim = 768  # all-mpnet-base-v2ì˜ ì°¨ì›
    hidden = hp.get('hidden', 128)
    num_blocks = hp.get('layers', 2)
    dropout = hp.get('dropout', 0.1)
else:
    # ê¸°ë³¸ê°’ ì‚¬ìš©
    in_dim = 768
    hidden = 128
    num_blocks = 2
    dropout = 0.1
    print("âš ï¸  ì²´í¬í¬ì¸íŠ¸ì— hp ì •ë³´ê°€ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# state_dict ì¶”ì¶œ
if 'state_dict' in ckpt:
    state_dict = ckpt['state_dict']
else:
    state_dict = ckpt

# ì¶œë ¥ í´ëž˜ìŠ¤ ìˆ˜ëŠ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ í™•ì¸
if 'head.weight' in state_dict:
    num_classes = state_dict['head.weight'].shape[0]
    print(f"ðŸ“Š ì²´í¬í¬ì¸íŠ¸ì—ì„œ num_classes í™•ì¸: {num_classes}")
elif 'label_encoder_classes' in ckpt:
    num_classes = len(ckpt['label_encoder_classes'])
    print(f"ðŸ“Š ì²´í¬í¬ì¸íŠ¸ì—ì„œ label_encoder_classesë¡œ num_classes í™•ì¸: {num_classes}")
elif meta.get('classes'):
    num_classes = len(meta['classes'])
    print(f"ðŸ“Š ë©”íƒ€ë°ì´í„°ì—ì„œ num_classes í™•ì¸: {num_classes}")
else:
    num_classes = 10  # ê¸°ë³¸ê°’
    print(f"âš ï¸  num_classesë¥¼ í™•ì¸í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©: {num_classes}")

print(f"ðŸ“Š ëª¨ë¸ ì„¤ì •: in_dim={in_dim}, hidden={hidden}, num_classes={num_classes}, num_blocks={num_blocks}, dropout={dropout}")

# ResGCN ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = ResGCN(in_dim=in_dim, hidden=hidden, out_dim=num_classes, layers=num_blocks, dropout=dropout)

# state_dict ë¡œë“œ
model.load_state_dict(state_dict)
print("âœ… ëª¨ë¸ state_dict ë¡œë“œ ì™„ë£Œ")

model.to(device)
model.eval()
print(f"âœ… ResGCN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})")

# Label Encoder ì„¤ì • (ì²´í¬í¬ì¸íŠ¸ ë˜ëŠ” ë©”íƒ€ë°ì´í„°ì—ì„œ)
if 'label_encoder_classes' in ckpt:
    label_encoder_classes = ckpt['label_encoder_classes']
elif meta.get('classes'):
    label_encoder_classes = meta['classes']
else:
    # ê¸°ë³¸ í´ëž˜ìŠ¤ ëª©ë¡ (ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•œ 10ê°œ í´ëž˜ìŠ¤)
    label_encoder_classes = [
        "Activity Notifications",
        "Confirmshaming",
        "Countdown Timers",
        "High-demand Messages",
        "Limited-time Messages",
        "Low-stock Messages",
        "Not Dark Pattern",
        "Pressured Selling",
        "Testimonials of Uncertain Origin",
        "Trick Questions"
    ]
    print("âš ï¸  Label encoder í´ëž˜ìŠ¤ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©")

# LabelEncoder ìƒì„± (ì˜ˆì¸¡ ê²°ê³¼ ë””ì½”ë”©ìš©)
label_encoder = LabelEncoder()
label_encoder.classes_ = np.array(label_encoder_classes)
print(f"âœ… Label Encoder ì„¤ì • ì™„ë£Œ: {len(label_encoder_classes)}ê°œ í´ëž˜ìŠ¤")

# Predicate -> Type ë§¤í•‘ (ì‚¬ìš©ìž ì œê³µ ë§¤í•‘)
PREDICATE_TO_TYPE_MAP = {
    # Urgency
    "Countdown Timers": "Urgency",
    "Limited-time Messages": "Urgency",
    # Misdirection
    "Confirmshaming": "Misdirection",
    "Trick Questions": "Misdirection",
    "Pressured Selling": "Misdirection",
    # Social Proof
    "Activity Notifications": "Social Proof",
    "Testimonials of Uncertain Origin": "Social Proof",
    # Scarcity
    "Low-stock Messages": "Scarcity",
    "High-demand Messages": "Scarcity",
    # Not Dark Pattern
    "Not Dark Pattern": "Not Dark Pattern",
}

def get_type_from_predicate(predicate):
    """
    Predicate ê°’ìœ¼ë¡œë¶€í„° Typeì„ ë°˜í™˜
    """
    if not predicate:
        return None
    # ì§ì ‘ ë§¤í•‘ í™•ì¸
    if predicate in PREDICATE_TO_TYPE_MAP:
        return PREDICATE_TO_TYPE_MAP[predicate]
    # ëŒ€ì†Œë¬¸ìž ë¬´ì‹œ ë§¤ì¹­
    predicate_lower = predicate.lower()
    for key, value in PREDICATE_TO_TYPE_MAP.items():
        if key.lower() == predicate_lower:
            return value
    return None

# í…ìŠ¤íŠ¸ ë¸”ë¡ íŒŒì‹± ìœ í‹¸ë¦¬í‹°
def parse_text_blocks(raw_text):
    """
    '#'(ë¸”ë¡) ë° '*'(ë‹¨ì–´) êµ¬ë¶„ìžë¥¼ ì‚¬ìš©í•˜ëŠ” ë¬¸ìžì—´ì„ ìžì—°ì–´ ë¬¸ìž¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    ê¸°ì¡´ í¬ë§·(*ë§Œ ì‚¬ìš©)ë„ ìžë™ìœ¼ë¡œ ì²˜ë¦¬
    """
    if raw_text is None:
        return []

    if isinstance(raw_text, list):
        candidates = raw_text
    else:
        text = str(raw_text)
        if "#" in text:
            candidates = [seg.strip() for seg in text.split("#") if seg.strip()]
        else:
            candidates = [seg.strip() for seg in text.split("*") if seg.strip()]

    cleaned = []
    for segment in candidates:
        if segment is None:
            continue
        segment_str = str(segment)
        segment_str = segment_str.replace("*", " ")
        segment_str = re.sub(r"\s+", " ", segment_str).strip()
        if segment_str:
            cleaned.append(segment_str)
    return cleaned

# kNN ê·¸ëž˜í”„ êµ¬ì„± í•¨ìˆ˜ (ë…¸íŠ¸ë¶ êµ¬ì¡°)
def knn_indices(emb, k=10, metric="cosine"):
    """kNN ì¸ë±ìŠ¤ ê³„ì‚°"""
    nn = NearestNeighbors(n_neighbors=k+1, metric=metric)
    nn.fit(emb)
    _, idx = nn.kneighbors(emb)
    return idx[:, 1:]  # drop self

def build_edge_index(neigh_idx: np.ndarray, mutual: bool):
    """ì—£ì§€ ì¸ë±ìŠ¤ êµ¬ì„± (ë…¸íŠ¸ë¶ êµ¬ì¡°)"""
    N, k = neigh_idx.shape
    rows = np.repeat(np.arange(N), k)
    cols = neigh_idx.reshape(-1)
    # mutual/non-mutual ëŒ€ì¹­ ì²˜ë¦¬
    if not mutual:
        ei = np.vstack([np.concatenate([rows, cols]),
                        np.concatenate([cols, rows])])
        return np.unique(ei, axis=1)
    # mutual kNN
    S = set(zip(rows.tolist(), cols.tolist()))
    mutual_pairs = [(i, j) for (i, j) in S if (j, i) in S and i != j]
    if len(mutual_pairs) == 0:
        ei = np.vstack([np.concatenate([rows, cols]),
                        np.concatenate([cols, rows])])
        return np.unique(ei, axis=1)
    r = np.array([p[0] for p in mutual_pairs])
    c = np.array([p[1] for p in mutual_pairs])
    ei = np.vstack([np.concatenate([r, c]),
                    np.concatenate([c, r])])
    return np.unique(ei, axis=1)

def forward_on_concat(model, X_train: np.ndarray, X_query: np.ndarray):
    """
    Inductive inference: train + query ìž„ë² ë”©ì„ concatí•˜ì—¬ kNN ê·¸ëž˜í”„ êµ¬ì„± í›„ ì¶”ë¡ 
    ë…¸íŠ¸ë¶ì˜ forward_on_concat ë°©ì‹ê³¼ ë™ì¼
    """
    if X_train is None or len(X_train) == 0:
        # Train embeddingsê°€ ì—†ìœ¼ë©´ ë‹¨ì¼ ë…¸ë“œ ê·¸ëž˜í”„ë¡œ ì¶”ë¡  (ë¹„ê¶Œìž¥)
        print("âš ï¸  Train embeddingsê°€ ì—†ì–´ ë‹¨ì¼ ë…¸ë“œ ê·¸ëž˜í”„ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.")
        X_cat = X_query
        # ë‹¨ì¼ ë…¸ë“œ ê·¸ëž˜í”„ (ì—£ì§€ ì—†ìŒ)
        edge_index = np.empty((2, 0), dtype=np.int64)
    else:
        # Train + Query concat
        X_cat = np.vstack([X_train, X_query])
        # kNN ê·¸ëž˜í”„ êµ¬ì„±
        knn_k = meta.get('knn_k', 10)
        metric = meta.get('metric', 'cosine')
        mutual_knn = meta.get('mutual_knn', True)
        
        knn = knn_indices(X_cat, k=knn_k, metric=metric)
        edge_index = build_edge_index(knn, mutual_knn)
    
    # PyG Data ê°ì²´ ìƒì„±
    data = Data(
        x=torch.tensor(X_cat, dtype=torch.float32, device=device),
        edge_index=torch.tensor(edge_index, dtype=torch.long, device=device),
    )
    
    # ì¶”ë¡ 
    model.eval()
    with torch.no_grad():
        logits = model(data)  # [total_nodes, num_classes]
        probs = F.softmax(logits, dim=1).detach().cpu().numpy()
    
    # Query ë¶€ë¶„ë§Œ ë°˜í™˜
    if X_train is not None and len(X_train) > 0:
        return probs[len(X_train):]
    else:
        return probs

# ì˜ˆì¸¡ í•¨ìˆ˜ (ë‘ ë‹¨ê³„ ë¶„ê¸° + ë²ˆì—­ í¬í•¨)
def process_image_and_predict(image_path):
    law_path = os.path.join(MODEL_DIR, "predicate_type_law.csv")
    if os.path.exists(law_path):
        laws_df = pd.read_csv(law_path)
        # predicate, type, laws ëª¨ë‘ í¬í•¨í•´ì•¼ í•¨ (predicateë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•´)
        reduced_law = laws_df[['predicate', 'type', 'laws']].drop_duplicates().reset_index(drop=True)
    else:
        reduced_law = pd.DataFrame(columns=['predicate', 'type', 'laws'])

    ocr_results = reader.readtext(image_path)
    output = []

    for (bbox, text, prob) in ocr_results:
        x_min = int(min(p[0] for p in bbox))
        y_min = int(min(p[1] for p in bbox))
        width = int(max(p[0] for p in bbox)) - x_min
        height = int(max(p[1] for p in bbox)) - y_min

        # ë²ˆì—­: ì˜ì–´ â†’ í•œêµ­ì–´
        input_text = text.strip()
        try:
            trans_inputs = trans_tokenizer.encode(input_text, return_tensors="pt", truncation=True).to(device)
            translated = trans_model.generate(trans_inputs, max_length=100)
            translated_text = trans_tokenizer.decode(translated[0], skip_special_tokens=True)
        except Exception:
            translated_text = input_text  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€

        # ResGCN ëª¨ë¸ë¡œ ì§ì ‘ ì˜ˆì¸¡ (1-2ë‹¨ê³„ êµ¬ë¶„ ì—†ì´)
        category, predicate, top_preds = None, None, []
        is_dark = 0
        probability = None

        try:
            # SentenceTransformerë¡œ ìž„ë² ë”© ìƒì„±
            with torch.no_grad():
                embedding = st_model.encode([translated_text], convert_to_numpy=True, show_progress_bar=False)  # [1, 768]
            
            # Inductive inference: forward_on_concat ì‚¬ìš©
            query_probs = forward_on_concat(model, X_train, embedding)  # [1, num_classes]
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            pred_probs = query_probs[0]  # [num_classes]
            pred_idx = np.argmax(pred_probs)
            
            # Predicate ë””ì½”ë”©
            predicate = label_encoder.inverse_transform([pred_idx])[0]
            probability = float(pred_probs[pred_idx])
            
            # ë‹¤í¬íŒ¨í„´ ì—¬ë¶€ íŒë‹¨: predicateê°€ "Not Dark Pattern"ì´ ì•„ë‹ˆë©´ ë‹¤í¬íŒ¨í„´
            is_not_dark_keywords = ["not dark pattern", "not_dark_pattern", "not dark", "normal", "none"]
            is_dark = 1 if not any(keyword in predicate.lower() for keyword in is_not_dark_keywords) else 0
            
            # Top 3 predictions
            top_indices = pred_probs.argsort()[::-1][:3]
            top_preds = [
                f"{label_encoder.inverse_transform([i])[0]} ({round(pred_probs[i], 4)})"
                for i in top_indices
            ]
            
            # CategoryëŠ” predicateë¡œë¶€í„° ë§¤í•‘ (ìš°ì„ : ì§ì ‘ ë§¤í•‘, ì—†ìœ¼ë©´ CSVì—ì„œ ì°¾ê¸°)
            if predicate:
                # ì§ì ‘ ë§¤í•‘ ì‚¬ìš© (ì‚¬ìš©ìž ì œê³µ ë§¤í•‘)
                category = get_type_from_predicate(predicate)
                # CSVì—ì„œ ì°¾ê¸° (fallback)
                if not category:
                    category_row = reduced_law[reduced_law["predicate"] == predicate]
                    if not category_row.empty:
                        category = category_row.iloc[0]["type"]
                # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ None ìœ ì§€
        except Exception as e:
            print(f"[WARNING] ResGCN ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            predicate = None
            top_preds = []
            category = None
            probability = None
            is_dark = 0

        # ë²•ë¥  ì •ë³´ ì—°ê²°
        law_list = []
        if category:
            law_row = reduced_law[reduced_law["type"] == category]
            if not law_row.empty:
                try:
                    law_list = json.loads(law_row.iloc[0]["laws"])
                except Exception as e:
                    print(f"[WARNING] JSON parsing error in laws: {e}")

        output.append({
            "text": text,
            "translated": translated_text,
            "confidence": float(prob),
            "bbox": json.dumps({"x": x_min, "y": y_min, "width": width, "height": height}),
            "is_darkpattern": is_dark,
            "predicate": predicate,
            "top1_predicate": top_preds[0] if len(top_preds) > 0 else None,
            "top2_predicate": top_preds[1] if len(top_preds) > 1 else None,
            "top3_predicate": top_preds[2] if len(top_preds) > 2 else None,
            "category": category,
            "type": category,
            "laws": law_list
        })

    return output

# í…ìŠ¤íŠ¸ ê¸°ë°˜ ì˜ˆì¸¡ í•¨ìˆ˜ (* ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬)
def process_text_and_predict(full_text, progress_callback=None):
    """
    fullTextë¥¼ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
    (ì‹ ê·œ í¬ë§·: '#' êµ¬ë¶„, ê¸°ì¡´ í¬ë§·: '*' êµ¬ë¶„)
    
    Args:
        full_text: ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ (ë¬¸ìžì—´ ë˜ëŠ” ë¬¸ìžì—´ ë¦¬ìŠ¤íŠ¸)
        
    Returns:
        ê° í…ìŠ¤íŠ¸ë³„ ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    law_path = os.path.join(MODEL_DIR, "predicate_type_law.csv")
    if os.path.exists(law_path):
        laws_df = pd.read_csv(law_path)
        # predicate, type, laws ëª¨ë‘ í¬í•¨í•´ì•¼ í•¨ (predicateë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•´)
        reduced_law = laws_df[['predicate', 'type', 'laws']].drop_duplicates().reset_index(drop=True)
    else:
        reduced_law = pd.DataFrame(columns=['predicate', 'type', 'laws'])
    
    # í…ìŠ¤íŠ¸ ë¸”ë¡ íŒŒì‹±
    text_list = parse_text_blocks(full_text)
    print(f"ðŸ“Š [í…ìŠ¤íŠ¸ ë¶„ë¦¬] ì´ {len(text_list)}ê°œ ë¸”ë¡ ì²˜ë¦¬ ì˜ˆì •")
    output = []
    
    for idx, text in enumerate(text_list, 1):
        input_text = text.strip()
        if not input_text:
            continue
        
        # ì§„í–‰ ìƒí™© ì½œë°± í˜¸ì¶œ (ìžˆëŠ” ê²½ìš°)
        if progress_callback:
            try:
                progress_callback(idx, len(text_list))
            except Exception as e:
                print(f"âš ï¸ [ì§„í–‰ ìƒí™© ì½œë°± ì˜¤ë¥˜] {str(e)}")
        
        # ì§„í–‰ ìƒí™© ë¡œê·¸ (ëª¨ë“  ë‹¨ê³„ì—ì„œ ì¶œë ¥)
        print(f"  ðŸ”„ [{idx}/{len(text_list)}] ëª¨ë¸ë§ ì§„í–‰ ì¤‘ ({input_text[:50]})")
        sys.stdout.flush()  # ë²„í¼ ê°•ì œ ì¶œë ¥
        
        # fullTextëŠ” ì´ë¯¸ í¬ë¡¬ ìµìŠ¤í…ì…˜ì—ì„œ ë²ˆì—­ëœ ì˜ì–´ í…ìŠ¤íŠ¸
        # ëª¨ë¸ì— ë“¤ì–´ê°€ëŠ” í…ìŠ¤íŠ¸ëŠ” ë°˜ë“œì‹œ ì˜ì–´ì—¬ì•¼ í•¨
        translated_text = input_text  # ì´ë¯¸ ë²ˆì—­ëœ í…ìŠ¤íŠ¸
        
        # í•œê¸€ ê°ì§€ ë° ê²½ê³  (ëª¨ë¸ì— í•œê¸€ì´ ë“¤ì–´ê°€ë©´ ì•ˆ ë¨)
        import re
        has_korean = bool(re.search(r'[ê°€-íž£]', translated_text))
        if has_korean:
            print(f"     âš ï¸ [ê²½ê³ ] ëª¨ë¸ì— í•œê¸€ í…ìŠ¤íŠ¸ê°€ ìž…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ë²ˆì—­ í™•ì¸ í•„ìš”)")
            print(f"     ìž…ë ¥ í…ìŠ¤íŠ¸: {translated_text[:100]}")
            sys.stdout.flush()
        
        category, predicate, probability, top_preds = None, None, None, []
        is_dark = 0
        
        # ResGCN ëª¨ë¸ë¡œ ì§ì ‘ ì˜ˆì¸¡ (ë…¸íŠ¸ë¶ êµ¬ì¡°: inductive inference)
        print(f"     ðŸ“Š ResGCN ëª¨ë¸ ì˜ˆì¸¡ ì¤‘ (ìž…ë ¥: {len(translated_text)}ìž)")
        sys.stdout.flush()
        
        try:
            # SentenceTransformerë¡œ ìž„ë² ë”© ìƒì„±
            with torch.no_grad():
                embedding = st_model.encode([translated_text], convert_to_numpy=True, show_progress_bar=False)  # [1, 768]
            
            # Inductive inference: forward_on_concat ì‚¬ìš© (ë…¸íŠ¸ë¶ ë°©ì‹)
            query_probs = forward_on_concat(model, X_train, embedding)  # [1, num_classes]
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            pred_probs = query_probs[0]  # [num_classes]
            pred_idx = np.argmax(pred_probs)
            
            # Predicate ë””ì½”ë”©
            predicate = label_encoder.inverse_transform([pred_idx])[0]
            probability = float(pred_probs[pred_idx])
            
            # ë‹¤í¬íŒ¨í„´ ì—¬ë¶€ íŒë‹¨: predicateê°€ "Not Dark Pattern"ì´ ì•„ë‹ˆë©´ ë‹¤í¬íŒ¨í„´
            is_not_dark_keywords = ["not dark pattern", "not_dark_pattern", "not dark", "normal", "none"]
            is_dark = 1 if not any(keyword in predicate.lower() for keyword in is_not_dark_keywords) else 0
            
            # Top 3 predictions
            top_indices = pred_probs.argsort()[::-1][:3]
            top_preds = [
                f"{label_encoder.inverse_transform([i])[0]} ({round(pred_probs[i], 4)})"
                for i in top_indices
            ]
            
            # CategoryëŠ” predicateë¡œë¶€í„° ë§¤í•‘ (ìš°ì„ : ì§ì ‘ ë§¤í•‘, ì—†ìœ¼ë©´ CSVì—ì„œ ì°¾ê¸°)
            if predicate:
                # ì§ì ‘ ë§¤í•‘ ì‚¬ìš© (ì‚¬ìš©ìž ì œê³µ ë§¤í•‘)
                category = get_type_from_predicate(predicate)
                # CSVì—ì„œ ì°¾ê¸° (fallback)
                if not category:
                    category_row = reduced_law[reduced_law["predicate"] == predicate]
                    if not category_row.empty:
                        category = category_row.iloc[0]["type"]
                # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ None ìœ ì§€
            
            # ê²°ê³¼ ë¡œê·¸
            if is_dark:
                print(f"     ðŸ”´ ë‹¤í¬íŒ¨í„´ ê°ì§€: Type={category}, Predicate={predicate}, í™•ë¥ ={round(probability*100, 1)}%")
            else:
                print(f"     âšª ì¼ë°˜ í…ìŠ¤íŠ¸: Predicate={predicate}, í™•ë¥ ={round(probability*100, 1)}%")
            sys.stdout.flush()
            
        except Exception as e:
            print(f"     âŒ ResGCN ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.stdout.flush()
            predicate = None
            probability = None
            top_preds = []
            category = None
            is_dark = 0
        
        # ë²•ë¥  ì •ë³´ ì—°ê²°
        law_list = []
        if category:
            law_row = reduced_law[reduced_law["type"] == category]
            if not law_row.empty:
                try:
                    law_list = json.loads(law_row.iloc[0]["laws"])
                except Exception as e:
                    print(f"[WARNING] JSON parsing error in laws: {e}")
        
        output.append({
            "text": translated_text,  # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ (ëª¨ë¸ë§ì— ì‚¬ìš©ëœ í…ìŠ¤íŠ¸)
            "translated": translated_text,  # í˜¸í™˜ì„± ìœ ì§€
            "is_darkpattern": is_dark,
            "predicate": predicate,
            "probability": probability,
            "top1_predicate": top_preds[0] if len(top_preds) > 0 else None,
            "top2_predicate": top_preds[1] if len(top_preds) > 1 else None,
            "top3_predicate": top_preds[2] if len(top_preds) > 2 else None,
            "category": category,
            "type": category,
            "laws": law_list
        })
    
    return output